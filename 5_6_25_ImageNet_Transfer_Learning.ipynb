{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jared-ni/cs2241-final/blob/main/5_6_25_ImageNet_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9xC0ezTPjeeN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZKVcuhrtJWx",
        "outputId": "b5f56509-7f07-41b1-b8dc-15379463a4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5-48L1Sromn",
        "outputId": "92e1f824-ae2a-4c0c-b670-3d7afc6917ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "Found GPU: /physical_device:GPU:0. Training will be accelerated.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import kagglehub\n",
        "import os\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    print(f\"Found GPU: {gpu_devices[0].name}. Training will be accelerated.\")\n",
        "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "else:\n",
        "    print(\"No GPU found. Training will run on CPU (might be slow).\")\n",
        "    print(\"Runtime > Change runtime type > Hardware accelerator > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czyE3UlPp-z0",
        "outputId": "87d9d11a-8449-4316-d4a7-16482a799194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/dog-and-cat-classification-dataset/PetImages\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "data_root = kagglehub.dataset_download(\"bhavikjikadara/dog-and-cat-classification-dataset\")\n",
        "\n",
        "data_dir = os.path.join(data_root, 'PetImages')\n",
        "\n",
        "print(\"Path to dataset files:\", data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QPku-jcSCICl"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COZY8nDOCnNT",
        "outputId": "e783786e-9bb6-4b4a-9259-e5009650b227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to output files: {'train': '/kaggle/split_data/train', 'val': '/kaggle/split_data/val', 'test': '/kaggle/split_data/test'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying Cat to train: 100%|██████████| 9999/9999 [01:13<00:00, 136.18it/s]\n",
            "Copying Cat to val: 100%|██████████| 1250/1250 [00:09<00:00, 137.70it/s]\n",
            "Copying Cat to test: 100%|██████████| 1250/1250 [00:08<00:00, 141.84it/s]\n",
            "Copying Dog to train: 100%|██████████| 9999/9999 [01:07<00:00, 147.17it/s]\n",
            "Copying Dog to val: 100%|██████████| 1250/1250 [00:08<00:00, 146.75it/s]\n",
            "Copying Dog to test: 100%|██████████| 1250/1250 [00:08<00:00, 140.13it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "output_base_dir = \"/kaggle/split_data\"\n",
        "\n",
        "# Output folders\n",
        "output_dirs = {\n",
        "    'train': os.path.join(output_base_dir, 'train'),\n",
        "    'val': os.path.join(output_base_dir, 'val'),\n",
        "    'test': os.path.join(output_base_dir, 'test')\n",
        "}\n",
        "\n",
        "# Classes (folder names in PetImages/)\n",
        "classes = ['Cat', 'Dog']\n",
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "\n",
        "# Create output folders\n",
        "for split in output_dirs:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(output_dirs[split], cls), exist_ok=True)\n",
        "\n",
        "print(\"Path to output files:\", output_dirs)\n",
        "# Split images\n",
        "for cls in classes:\n",
        "    src_folder = os.path.join(data_dir, cls)\n",
        "    all_files = [f for f in os.listdir(src_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    train_cutoff = int(len(all_files) * split_ratio[0])\n",
        "    val_cutoff = int(len(all_files) * (split_ratio[0] + split_ratio[1]))\n",
        "\n",
        "    splits = {\n",
        "        'train': all_files[:train_cutoff],\n",
        "        'val': all_files[train_cutoff:val_cutoff],\n",
        "        'test': all_files[val_cutoff:]\n",
        "    }\n",
        "\n",
        "    for split, files in splits.items():\n",
        "        for f in tqdm(files, desc=f\"Copying {cls} to {split}\"):\n",
        "            src = os.path.join(src_folder, f)\n",
        "            dst = os.path.join(output_dirs[split], cls, f)\n",
        "            try:\n",
        "                shutil.copyfile(src, dst)\n",
        "            except Exception as e:\n",
        "                pass  # Skip corrupted files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGENET FEATURE MAP EXTRACTION (512, 7, 7)\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Load VGG16 pre-trained on ImageNet\n",
        "print(\"Loading VGG16 pre-trained on ImageNet...\")\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Get the convolutional base (features)\n",
        "spatial_feature_extractor_vgg = vgg16.features\n",
        "\n",
        "# Freeze the weights of the pre-trained layers\n",
        "for param in spatial_feature_extractor_vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Set the model to evaluation mode (important for dropout if it were present, or batch norm if it were ResNet etc.)\n",
        "spatial_feature_extractor_vgg.eval() # Do this once after setup\n",
        "\n",
        "print(\"\\nVGG16 Spatial Feature Extractor created and set to eval mode.\")\n",
        "print(f\"Conceptual output shape (spatial features for 224x224 input): (Batch_Size, 512, 7, 7)\")\n",
        "\n",
        "img_height, img_width = 224, 224\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    # 1. Resize the image\n",
        "    transforms.Resize((img_height, img_width)),\n",
        "    # 2. Convert PIL Image to PyTorch Tensor\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhmi5szxbQdH",
        "outputId": "8ffbec09-5a52-4968-91b6-530db67817a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VGG16 pre-trained on ImageNet...\n",
            "Model loaded.\n",
            "\n",
            "VGG16 Spatial Feature Extractor created and set to eval mode.\n",
            "Conceptual output shape (spatial features for 224x224 input): (Batch_Size, 512, 7, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_image_path(image_path: str) -> torch.Tensor:\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
        "    try:\n",
        "        # Load the image using PIL\n",
        "        img_pil = Image.open(image_path)\n",
        "        if img_pil.mode != 'RGB':\n",
        "            img_pil = img_pil.convert('RGB')\n",
        "        processed_img_th = preprocess(img_pil)\n",
        "        input_tensor = processed_img_th.unsqueeze(0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        raise\n",
        "    with torch.no_grad():\n",
        "        spatial_features = spatial_feature_extractor_vgg(input_tensor)\n",
        "    return spatial_features\n",
        "\n",
        "my_image_path = '/kaggle/split_data/test/Cat/100.jpg'\n",
        "stats_output_file = 'feature_map_stats.txt'\n",
        "full_feature_map_file = 'full_feature_map.txt'\n",
        "\n",
        "if not os.path.exists(my_image_path):\n",
        "    print(f\"Example image not found at {my_image_path}. Please update the path.\")\n",
        "else:\n",
        "    print(f\"\\n--- Extracting features for {my_image_path} ---\")\n",
        "\n",
        "# get the feature map\n",
        "feature_map = extract_features_from_image_path(my_image_path)\n",
        "\n",
        "# Print the shape and some info to console\n",
        "print(f\"\\nSuccessfully extracted feature map.\")\n",
        "print(f\"Feature map shape: {feature_map.shape}\")\n",
        "print(f\"Feature map dtype: {feature_map.dtype}\")\n",
        "\n",
        "# --- Calculate summary statistics ---\n",
        "min_val = torch.min(feature_map)\n",
        "max_val = torch.max(feature_map)\n",
        "mean_val = torch.mean(feature_map)\n",
        "std_val = torch.std(feature_map)\n",
        "\n",
        "# --- Print summary statistics to console ---\n",
        "print(\"\\n--- Summary statistics of the extracted feature map ---\")\n",
        "print(f\"Min value: {min_val}\")\n",
        "print(f\"Max value: {max_val}\")\n",
        "print(f\"Mean value: {mean_val}\")\n",
        "print(f\"Standard deviation: {std_val}\")\n",
        "\n",
        "# --- Optional: Print a slice to console ---\n",
        "print(\"\\n--- Slice of the feature map (Batch 0, first 5 channels, H=0, W=0) ---\")\n",
        "print(feature_map[0, :5, 0, 0])\n",
        "\n",
        "\n",
        "# --- Write summary statistics to the stats text file ---\n",
        "print(f\"\\nWriting summary statistics to {stats_output_file}...\")\n",
        "with open(stats_output_file, 'w') as f:\n",
        "    f.write(f\"Feature Map Statistics for Image: {my_image_path}\\n\")\n",
        "    f.write(\"-\" * 40 + \"\\n\")\n",
        "    f.write(f\"Feature map shape: {feature_map.shape}\\n\")\n",
        "    f.write(f\"Feature map dtype: {feature_map.dtype}\\n\")\n",
        "    f.write(\"-\" * 40 + \"\\n\")\n",
        "    f.write(f\"Min value: {min_val.item()}\\n\") # Use .item() to get scalar value\n",
        "    f.write(f\"Max value: {max_val.item()}\\n\")\n",
        "    f.write(f\"Mean value: {mean_val.item()}\\n\")\n",
        "    f.write(f\"Standard deviation: {std_val.item()}\\n\")\n",
        "    f.write(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "    print(f\"Summary statistics successfully written to {stats_output_file}.\")\n",
        "    print(f\"\\nWriting the FULL feature map content to {full_feature_map_file}...\")\n",
        "\n",
        "np.save(\"feature_map_nparray.npy\", feature_map.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmcYYmUKhTjN",
        "outputId": "224ae6e2-bff9-465a-dca5-b98aa7f70b91"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extracting features for /kaggle/split_data/test/Cat/100.jpg ---\n",
            "\n",
            "Successfully extracted feature map.\n",
            "Feature map shape: torch.Size([1, 512, 7, 7])\n",
            "Feature map dtype: torch.float32\n",
            "\n",
            "--- Summary statistics of the extracted feature map ---\n",
            "Min value: 0.0\n",
            "Max value: 21.97066307067871\n",
            "Mean value: 0.37017399072647095\n",
            "Standard deviation: 1.1964044570922852\n",
            "\n",
            "--- Slice of the feature map (Batch 0, first 5 channels, H=0, W=0) ---\n",
            "tensor([1.7278, 2.6705, 0.0000, 0.0000, 2.5132])\n",
            "\n",
            "Writing summary statistics to feature_map_stats.txt...\n",
            "Summary statistics successfully written to feature_map_stats.txt.\n",
            "\n",
            "Writing the FULL feature map content to full_feature_map.txt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGENET 4096 FEATURE EXTRACTION SETUP\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image # Import PIL for loading images\n",
        "import numpy as np\n",
        "import os\n",
        "import traceback # Import traceback for detailed error printing\n",
        "\n",
        "# 1. Load VGG16 pre-trained on ImageNet\n",
        "print(\"Loading VGG16 pre-trained on ImageNet...\")\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# 2. Create the feature extractor as a custom Module, explicitly handling Flatten\n",
        "class VGG16FeatureExtractor4096(nn.Module):\n",
        "    def __init__(self, original_vgg16):\n",
        "        super().__init__()\n",
        "        # Keep the convolutional base (output is 512x7x7 for 224x224 input)\n",
        "        self.features = original_vgg16.features\n",
        "\n",
        "        # Manually add the Flatten layer\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "             original_vgg16.classifier[0], # First Linear layer (25088 -> 4096)\n",
        "             original_vgg16.classifier[1]  # ReLU after it\n",
        "        )\n",
        "\n",
        "        for param in original_vgg16.parameters():\n",
        "             param.requires_grad = False\n",
        "    def forward(self, x):\n",
        "        # Pass through the convolutional base\n",
        "        x = self.features(x) # Output shape: (Batch, 512, 7, 7)\n",
        "        x = self.flatten(x) # Expected output shape: (Batch, 512 * 7 * 7) = (Batch, 25088)\n",
        "        x = self.classifier_head(x) # Output shape: (Batch, 4096)\n",
        "        return x\n",
        "\n",
        "# Instantiate the custom feature extractor\n",
        "feature_extractor_vgg_4096 = VGG16FeatureExtractor4096(vgg16)\n",
        "feature_extractor_vgg_4096.eval()\n",
        "\n",
        "print(\"\\nVGG16 4096-element Feature Extractor (Custom Module V2) created and set to eval mode.\")\n",
        "print(f\"Conceptual output shape (4096 features for 224x224 input): (Batch_Size, 4096)\")\n",
        "\n",
        "img_height, img_width = 224, 224\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((img_height, img_width)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "print(\"\\nPreprocessing pipeline imagenet 4096 defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x0um4nwvEQv",
        "outputId": "945ef33f-6db8-4047-ca5f-81b7dffbf3f6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VGG16 pre-trained on ImageNet...\n",
            "Model loaded.\n",
            "\n",
            "VGG16 4096-element Feature Extractor (Custom Module V2) created and set to eval mode.\n",
            "Conceptual output shape (4096 features for 224x224 input): (Batch_Size, 4096)\n",
            "\n",
            "Preprocessing pipeline imagenet 4096 defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_4096_features_from_image_path(image_path: str) -> torch.Tensor:\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
        "    try:\n",
        "        img_pil = Image.open(image_path)\n",
        "        if img_pil.mode != 'RGB':\n",
        "            img_pil = img_pil.convert('RGB')\n",
        "        processed_img_th = preprocess(img_pil)\n",
        "        input_tensor = processed_img_th.unsqueeze(0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features_4096 = feature_extractor_vgg_4096(input_tensor)\n",
        "\n",
        "    return features_4096\n",
        "\n",
        "\n",
        "# Make sure you have an image file at this path!\n",
        "my_image_path = '/kaggle/split_data/test/Cat/100.jpg' # <--- Use your specific test path\n",
        "feature_vector_output_file = 'image_feature_vector_4096.txt' # <--- Define the output file path\n",
        "\n",
        "if not os.path.exists(my_image_path):\n",
        "    print(f\"Example image not found at {my_image_path}. Please update the path.\")\n",
        "else:\n",
        "    print(f\"\\n--- Extracting 4096 features for {my_image_path} ---\")\n",
        "    # Call the function to get the 4096 feature vector\n",
        "    feature_vector_4096 = extract_4096_features_from_image_path(my_image_path)\n",
        "\n",
        "    # Print the shape and some info\n",
        "    print(f\"\\nSuccessfully extracted 4096 feature vector.\")\n",
        "    print(f\"Feature vector shape: {feature_vector_4096.shape}\") # Should be (1, 4096)\n",
        "    print(f\"Feature vector dtype: {feature_vector_4096.dtype}\")\n",
        "\n",
        "    print(\"\\n--- Summary statistics of the extracted 4096 feature vector ---\")\n",
        "    print(f\"Min value: {torch.min(feature_vector_4096).item()}\")\n",
        "    print(f\"Max value: {torch.max(feature_vector_4096).item()}\")\n",
        "    print(f\"Mean value: {torch.mean(feature_vector_4096).item()}\")\n",
        "    print(f\"Standard deviation: {torch.std(feature_vector_4096).item()}\")\n",
        "\n",
        "    # --- Write the FULL 4096 feature vector to a text file ---\n",
        "    print(f\"\\nWriting the FULL 4096 feature vector to {feature_vector_output_file}...\")\n",
        "\n",
        "    # The tensor is (1, 4096). We want the 4096 values from the first (and only) item in the batch.\n",
        "    # Convert the tensor to a NumPy array, then flatten it to a 1D array, then convert to a list.\n",
        "    feature_list = feature_vector_4096.tolist()\n",
        "    feature_string = str(feature_list)\n",
        "\n",
        "    # Write this string representation to the file\n",
        "    with open(feature_vector_output_file, 'w') as f:\n",
        "        f.write(f\"Feature Vector (4096 elements) for Image: {my_image_path}\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        f.write(f\"Shape: {feature_vector_4096.shape}, Dtype: {feature_vector_4096.dtype}\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        # Write the single, potentially very long, string\n",
        "        f.write(feature_string)\n",
        "        f.write(\"\\n\") # Add a newline at the end\n",
        "    np.save(\"feature_vector_4096_nparray.npy\", feature_vector_4096.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAnRE2r1s-st",
        "outputId": "48069407-42f4-4b44-988e-3bd7645c3119"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extracting 4096 features for /kaggle/split_data/test/Cat/100.jpg ---\n",
            "\n",
            "Successfully extracted 4096 feature vector.\n",
            "Feature vector shape: torch.Size([1, 4096])\n",
            "Feature vector dtype: torch.float32\n",
            "\n",
            "--- Summary statistics of the extracted 4096 feature vector ---\n",
            "Min value: 0.0\n",
            "Max value: 6.057476043701172\n",
            "Mean value: 0.2343532145023346\n",
            "Standard deviation: 0.7326307892799377\n",
            "\n",
            "Writing the FULL 4096 feature vector to image_feature_vector_4096.txt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset_and_save_features(input_base_dir, output_base_dir):\n",
        "    \"\"\"\n",
        "    Iterates through the dataset directory structure, extracts features for each image,\n",
        "    and saves them as .npy files in a mirrored directory structure.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_base_dir):\n",
        "        print(f\"ERROR: Input dataset directory not found: {input_base_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nStarting feature extraction for dataset at: {input_base_dir}\")\n",
        "    print(f\"Features will be saved under: {output_base_dir}\")\n",
        "\n",
        "    # Ensure the main output directory exists\n",
        "    os.makedirs(output_base_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through 'test', 'train', 'val' subfolders\n",
        "    for split_name in os.listdir(input_base_dir):\n",
        "        split_dir = os.path.join(input_base_dir, split_name)\n",
        "        if not os.path.isdir(split_dir):\n",
        "            continue\n",
        "        print(f\"\\nProcessing split: {split_name}\")\n",
        "\n",
        "        # Create corresponding split directory in output\n",
        "        output_split_dir = os.path.join(output_base_dir, split_name)\n",
        "        os.makedirs(output_split_dir, exist_ok=True)\n",
        "\n",
        "        # Iterate through 'Cat', 'Dog' sub-subfolders\n",
        "        for class_name in os.listdir(split_dir):\n",
        "            class_dir = os.path.join(split_dir, class_name)\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "            print(f\"  Processing class: {class_name}\")\n",
        "\n",
        "            # Create corresponding class directory in output\n",
        "            output_class_dir = os.path.join(output_split_dir, class_name)\n",
        "            os.makedirs(output_class_dir, exist_ok=True)\n",
        "\n",
        "            # Iterate through images in the class directory\n",
        "            processed_in_class_count = 0\n",
        "            image_files_in_class = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
        "\n",
        "            for image_name in image_files_in_class:\n",
        "                image_path = os.path.join(class_dir, image_name)\n",
        "\n",
        "                # Check for valid image extensions\n",
        "                if not image_name.lower().endswith('jpg'):\n",
        "                    # print(f\"    Skipping non-image file: {image_name}\")\n",
        "                    continue\n",
        "\n",
        "                # Construct the output feature file path\n",
        "                base_name, _ = os.path.splitext(image_name)\n",
        "                feature_file_name = f\"{base_name}_feature_vector.npy\"\n",
        "                feature_file_path = os.path.join(output_class_dir, feature_file_name)\n",
        "\n",
        "                try:\n",
        "                    # Extract the 4096 feature vector (PyTorch tensor, shape (1, 4096))\n",
        "                    feature_vector_tensor = extract_4096_features_from_image_path(image_path)\n",
        "\n",
        "                    # Convert to NumPy array and remove the batch dimension\n",
        "                    # Squeeze(0) converts from (1, 4096) to (4096,)\n",
        "                    feature_vector_np = feature_vector_tensor.squeeze(0).numpy()\n",
        "\n",
        "                    # Save the NumPy array\n",
        "                    np.save(feature_file_path, feature_vector_np)\n",
        "\n",
        "                    processed_in_class_count += 1\n",
        "                    if processed_in_class_count % 50 == 0 or processed_in_class_count == len([f for f in image_files_in_class if f.lower().endswith('jpg')]): # Log progress\n",
        "                        valid_image_count = len([f for f in image_files_in_class if f.lower().endswith('jpg')])\n",
        "                        print(f\"    Saved features for {processed_in_class_count}/{valid_image_count} images in {split_name}/{class_name}...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    ERROR extracting/saving features for {image_name}: {e}\")\n",
        "                    # traceback.print_exc() # Uncomment for full traceback if needed for debugging\n",
        "\n",
        "            print(f\"  Finished processing class {class_name}. Extracted features for {processed_in_class_count} images.\")\n",
        "        print(f\"Finished processing split {split_name}.\")\n",
        "\n",
        "    print(\"\\nAll feature extraction complete.\")\n",
        "\n",
        "\n",
        "# Process the dataset\n",
        "OUTPUT_FEATURES_BASE_DIR = '/kaggle/feature_vectors'\n",
        "INPUT_DATA_BASE_DIR = '/kaggle/split_data'\n",
        "process_dataset_and_save_features(INPUT_DATA_BASE_DIR, OUTPUT_FEATURES_BASE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrk1X4R1--rH",
        "outputId": "83c7b032-32b3-4214-8503-5aa57efa26b4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting feature extraction for dataset at: /kaggle/split_data\n",
            "Features will be saved under: /kaggle/feature_vectors\n",
            "\n",
            "Processing split: val\n",
            "  Processing class: Dog\n",
            "    Saved features for 50/1250 images in val/Dog...\n",
            "    Saved features for 100/1250 images in val/Dog...\n",
            "    Saved features for 150/1250 images in val/Dog...\n",
            "    Saved features for 200/1250 images in val/Dog...\n",
            "    Saved features for 250/1250 images in val/Dog...\n",
            "    Saved features for 300/1250 images in val/Dog...\n",
            "    Saved features for 350/1250 images in val/Dog...\n",
            "    Saved features for 400/1250 images in val/Dog...\n",
            "    Saved features for 450/1250 images in val/Dog...\n",
            "    Saved features for 500/1250 images in val/Dog...\n",
            "    Saved features for 550/1250 images in val/Dog...\n",
            "    Saved features for 600/1250 images in val/Dog...\n",
            "    Saved features for 650/1250 images in val/Dog...\n",
            "    Saved features for 700/1250 images in val/Dog...\n",
            "    Saved features for 750/1250 images in val/Dog...\n",
            "    Saved features for 800/1250 images in val/Dog...\n",
            "    Saved features for 850/1250 images in val/Dog...\n",
            "    Saved features for 900/1250 images in val/Dog...\n",
            "    Saved features for 950/1250 images in val/Dog...\n",
            "    Saved features for 1000/1250 images in val/Dog...\n",
            "    Saved features for 1050/1250 images in val/Dog...\n",
            "    Saved features for 1100/1250 images in val/Dog...\n",
            "    Saved features for 1150/1250 images in val/Dog...\n",
            "    Saved features for 1200/1250 images in val/Dog...\n",
            "    Saved features for 1250/1250 images in val/Dog...\n",
            "  Finished processing class Dog. Extracted features for 1250 images.\n",
            "  Processing class: Cat\n",
            "    Saved features for 50/1250 images in val/Cat...\n",
            "    Saved features for 100/1250 images in val/Cat...\n",
            "    Saved features for 150/1250 images in val/Cat...\n",
            "    Saved features for 200/1250 images in val/Cat...\n",
            "    Saved features for 250/1250 images in val/Cat...\n",
            "    Saved features for 300/1250 images in val/Cat...\n",
            "    Saved features for 350/1250 images in val/Cat...\n",
            "    Saved features for 400/1250 images in val/Cat...\n",
            "    Saved features for 450/1250 images in val/Cat...\n",
            "    Saved features for 500/1250 images in val/Cat...\n",
            "    Saved features for 550/1250 images in val/Cat...\n",
            "    Saved features for 600/1250 images in val/Cat...\n",
            "    Saved features for 650/1250 images in val/Cat...\n",
            "    Saved features for 700/1250 images in val/Cat...\n",
            "    Saved features for 750/1250 images in val/Cat...\n",
            "    Saved features for 800/1250 images in val/Cat...\n",
            "    Saved features for 850/1250 images in val/Cat...\n",
            "    Saved features for 900/1250 images in val/Cat...\n",
            "    Saved features for 950/1250 images in val/Cat...\n",
            "    Saved features for 1000/1250 images in val/Cat...\n",
            "    Saved features for 1050/1250 images in val/Cat...\n",
            "    Saved features for 1100/1250 images in val/Cat...\n",
            "    Saved features for 1150/1250 images in val/Cat...\n",
            "    Saved features for 1200/1250 images in val/Cat...\n",
            "    Saved features for 1250/1250 images in val/Cat...\n",
            "  Finished processing class Cat. Extracted features for 1250 images.\n",
            "Finished processing split val.\n",
            "\n",
            "Processing split: test\n",
            "  Processing class: Dog\n",
            "    Saved features for 50/1250 images in test/Dog...\n",
            "    Saved features for 100/1250 images in test/Dog...\n",
            "    Saved features for 150/1250 images in test/Dog...\n",
            "    Saved features for 200/1250 images in test/Dog...\n",
            "    Saved features for 250/1250 images in test/Dog...\n",
            "    Saved features for 300/1250 images in test/Dog...\n",
            "    Saved features for 350/1250 images in test/Dog...\n",
            "    Saved features for 400/1250 images in test/Dog...\n",
            "    Saved features for 450/1250 images in test/Dog...\n",
            "    Saved features for 500/1250 images in test/Dog...\n",
            "    Saved features for 550/1250 images in test/Dog...\n",
            "    Saved features for 600/1250 images in test/Dog...\n",
            "    Saved features for 650/1250 images in test/Dog...\n",
            "    Saved features for 700/1250 images in test/Dog...\n",
            "    Saved features for 750/1250 images in test/Dog...\n",
            "    Saved features for 800/1250 images in test/Dog...\n",
            "    Saved features for 850/1250 images in test/Dog...\n",
            "    Saved features for 900/1250 images in test/Dog...\n",
            "    Saved features for 950/1250 images in test/Dog...\n",
            "    Saved features for 1000/1250 images in test/Dog...\n",
            "    Saved features for 1050/1250 images in test/Dog...\n",
            "    Saved features for 1100/1250 images in test/Dog...\n",
            "    Saved features for 1150/1250 images in test/Dog...\n",
            "    Saved features for 1200/1250 images in test/Dog...\n",
            "    Saved features for 1250/1250 images in test/Dog...\n",
            "  Finished processing class Dog. Extracted features for 1250 images.\n",
            "  Processing class: Cat\n",
            "    Saved features for 50/1250 images in test/Cat...\n",
            "    Saved features for 100/1250 images in test/Cat...\n",
            "    Saved features for 150/1250 images in test/Cat...\n",
            "    Saved features for 200/1250 images in test/Cat...\n",
            "    Saved features for 250/1250 images in test/Cat...\n",
            "    Saved features for 300/1250 images in test/Cat...\n",
            "    Saved features for 350/1250 images in test/Cat...\n",
            "    Saved features for 400/1250 images in test/Cat...\n",
            "    Saved features for 450/1250 images in test/Cat...\n",
            "    Saved features for 500/1250 images in test/Cat...\n",
            "    Saved features for 550/1250 images in test/Cat...\n",
            "    Saved features for 600/1250 images in test/Cat...\n",
            "    Saved features for 650/1250 images in test/Cat...\n",
            "    Saved features for 700/1250 images in test/Cat...\n",
            "    Saved features for 750/1250 images in test/Cat...\n",
            "    Saved features for 800/1250 images in test/Cat...\n",
            "    Saved features for 850/1250 images in test/Cat...\n",
            "    Saved features for 900/1250 images in test/Cat...\n",
            "    Saved features for 950/1250 images in test/Cat...\n",
            "    Saved features for 1000/1250 images in test/Cat...\n",
            "    Saved features for 1050/1250 images in test/Cat...\n",
            "    Saved features for 1100/1250 images in test/Cat...\n",
            "    Saved features for 1150/1250 images in test/Cat...\n",
            "    Saved features for 1200/1250 images in test/Cat...\n",
            "    Saved features for 1250/1250 images in test/Cat...\n",
            "  Finished processing class Cat. Extracted features for 1250 images.\n",
            "Finished processing split test.\n",
            "\n",
            "Processing split: train\n",
            "  Processing class: Dog\n",
            "    Saved features for 50/9999 images in train/Dog...\n",
            "    Saved features for 100/9999 images in train/Dog...\n",
            "    Saved features for 150/9999 images in train/Dog...\n",
            "    Saved features for 200/9999 images in train/Dog...\n",
            "    Saved features for 250/9999 images in train/Dog...\n",
            "    Saved features for 300/9999 images in train/Dog...\n",
            "    Saved features for 350/9999 images in train/Dog...\n",
            "    Saved features for 400/9999 images in train/Dog...\n",
            "    Saved features for 450/9999 images in train/Dog...\n",
            "    Saved features for 500/9999 images in train/Dog...\n",
            "    Saved features for 550/9999 images in train/Dog...\n",
            "    Saved features for 600/9999 images in train/Dog...\n",
            "    Saved features for 650/9999 images in train/Dog...\n",
            "    Saved features for 700/9999 images in train/Dog...\n",
            "    Saved features for 750/9999 images in train/Dog...\n",
            "    Saved features for 800/9999 images in train/Dog...\n",
            "    Saved features for 850/9999 images in train/Dog...\n",
            "    Saved features for 900/9999 images in train/Dog...\n",
            "    Saved features for 950/9999 images in train/Dog...\n",
            "    Saved features for 1000/9999 images in train/Dog...\n",
            "    Saved features for 1050/9999 images in train/Dog...\n",
            "    Saved features for 1100/9999 images in train/Dog...\n",
            "    Saved features for 1150/9999 images in train/Dog...\n",
            "    Saved features for 1200/9999 images in train/Dog...\n",
            "    Saved features for 1250/9999 images in train/Dog...\n",
            "    Saved features for 1300/9999 images in train/Dog...\n",
            "    Saved features for 1350/9999 images in train/Dog...\n",
            "    Saved features for 1400/9999 images in train/Dog...\n",
            "    Saved features for 1450/9999 images in train/Dog...\n",
            "    Saved features for 1500/9999 images in train/Dog...\n",
            "    Saved features for 1550/9999 images in train/Dog...\n",
            "    Saved features for 1600/9999 images in train/Dog...\n",
            "    Saved features for 1650/9999 images in train/Dog...\n",
            "    Saved features for 1700/9999 images in train/Dog...\n",
            "    Saved features for 1750/9999 images in train/Dog...\n",
            "    Saved features for 1800/9999 images in train/Dog...\n",
            "    Saved features for 1850/9999 images in train/Dog...\n",
            "    Saved features for 1900/9999 images in train/Dog...\n",
            "    Saved features for 1950/9999 images in train/Dog...\n",
            "    Saved features for 2000/9999 images in train/Dog...\n",
            "    Saved features for 2050/9999 images in train/Dog...\n",
            "    Saved features for 2100/9999 images in train/Dog...\n",
            "    Saved features for 2150/9999 images in train/Dog...\n",
            "    Saved features for 2200/9999 images in train/Dog...\n",
            "    Saved features for 2250/9999 images in train/Dog...\n",
            "    Saved features for 2300/9999 images in train/Dog...\n",
            "    Saved features for 2350/9999 images in train/Dog...\n",
            "    Saved features for 2400/9999 images in train/Dog...\n",
            "    Saved features for 2450/9999 images in train/Dog...\n",
            "    Saved features for 2500/9999 images in train/Dog...\n",
            "    Saved features for 2550/9999 images in train/Dog...\n",
            "    Saved features for 2600/9999 images in train/Dog...\n",
            "    Saved features for 2650/9999 images in train/Dog...\n",
            "    Saved features for 2700/9999 images in train/Dog...\n",
            "    Saved features for 2750/9999 images in train/Dog...\n",
            "    Saved features for 2800/9999 images in train/Dog...\n",
            "    Saved features for 2850/9999 images in train/Dog...\n",
            "    Saved features for 2900/9999 images in train/Dog...\n",
            "    Saved features for 2950/9999 images in train/Dog...\n",
            "    Saved features for 3000/9999 images in train/Dog...\n",
            "    Saved features for 3050/9999 images in train/Dog...\n",
            "    Saved features for 3100/9999 images in train/Dog...\n",
            "    Saved features for 3150/9999 images in train/Dog...\n",
            "    Saved features for 3200/9999 images in train/Dog...\n",
            "    Saved features for 3250/9999 images in train/Dog...\n",
            "    Saved features for 3300/9999 images in train/Dog...\n",
            "    Saved features for 3350/9999 images in train/Dog...\n",
            "    Saved features for 3400/9999 images in train/Dog...\n",
            "    Saved features for 3450/9999 images in train/Dog...\n",
            "    Saved features for 3500/9999 images in train/Dog...\n",
            "    Saved features for 3550/9999 images in train/Dog...\n",
            "    Saved features for 3600/9999 images in train/Dog...\n",
            "    Saved features for 3650/9999 images in train/Dog...\n",
            "    Saved features for 3700/9999 images in train/Dog...\n",
            "    Saved features for 3750/9999 images in train/Dog...\n",
            "    Saved features for 3800/9999 images in train/Dog...\n",
            "    Saved features for 3850/9999 images in train/Dog...\n",
            "    Saved features for 3900/9999 images in train/Dog...\n",
            "    Saved features for 3950/9999 images in train/Dog...\n",
            "    Saved features for 4000/9999 images in train/Dog...\n",
            "    Saved features for 4050/9999 images in train/Dog...\n",
            "    Saved features for 4100/9999 images in train/Dog...\n",
            "    Saved features for 4150/9999 images in train/Dog...\n",
            "    Saved features for 4200/9999 images in train/Dog...\n",
            "    Saved features for 4250/9999 images in train/Dog...\n",
            "    Saved features for 4300/9999 images in train/Dog...\n",
            "    Saved features for 4350/9999 images in train/Dog...\n",
            "    Saved features for 4400/9999 images in train/Dog...\n",
            "    Saved features for 4450/9999 images in train/Dog...\n",
            "    Saved features for 4500/9999 images in train/Dog...\n",
            "    Saved features for 4550/9999 images in train/Dog...\n",
            "    Saved features for 4600/9999 images in train/Dog...\n",
            "    Saved features for 4650/9999 images in train/Dog...\n",
            "    Saved features for 4700/9999 images in train/Dog...\n",
            "    Saved features for 4750/9999 images in train/Dog...\n",
            "    Saved features for 4800/9999 images in train/Dog...\n",
            "    Saved features for 4850/9999 images in train/Dog...\n",
            "    Saved features for 4900/9999 images in train/Dog...\n",
            "    Saved features for 4950/9999 images in train/Dog...\n",
            "    Saved features for 5000/9999 images in train/Dog...\n",
            "    Saved features for 5050/9999 images in train/Dog...\n",
            "    Saved features for 5100/9999 images in train/Dog...\n",
            "    Saved features for 5150/9999 images in train/Dog...\n",
            "    Saved features for 5200/9999 images in train/Dog...\n",
            "    Saved features for 5250/9999 images in train/Dog...\n",
            "    Saved features for 5300/9999 images in train/Dog...\n",
            "    Saved features for 5350/9999 images in train/Dog...\n",
            "    Saved features for 5400/9999 images in train/Dog...\n",
            "    Saved features for 5450/9999 images in train/Dog...\n",
            "    Saved features for 5500/9999 images in train/Dog...\n",
            "    Saved features for 5550/9999 images in train/Dog...\n",
            "    Saved features for 5600/9999 images in train/Dog...\n",
            "    Saved features for 5650/9999 images in train/Dog...\n",
            "    Saved features for 5700/9999 images in train/Dog...\n",
            "    Saved features for 5750/9999 images in train/Dog...\n",
            "    Saved features for 5800/9999 images in train/Dog...\n",
            "    Saved features for 5850/9999 images in train/Dog...\n",
            "    Saved features for 5900/9999 images in train/Dog...\n",
            "    Saved features for 5950/9999 images in train/Dog...\n",
            "    Saved features for 6000/9999 images in train/Dog...\n",
            "    Saved features for 6050/9999 images in train/Dog...\n",
            "    Saved features for 6100/9999 images in train/Dog...\n",
            "    Saved features for 6150/9999 images in train/Dog...\n",
            "    Saved features for 6200/9999 images in train/Dog...\n",
            "    Saved features for 6250/9999 images in train/Dog...\n",
            "    Saved features for 6300/9999 images in train/Dog...\n",
            "    Saved features for 6350/9999 images in train/Dog...\n",
            "    Saved features for 6400/9999 images in train/Dog...\n",
            "    Saved features for 6450/9999 images in train/Dog...\n",
            "    Saved features for 6500/9999 images in train/Dog...\n",
            "    Saved features for 6550/9999 images in train/Dog...\n",
            "    Saved features for 6600/9999 images in train/Dog...\n",
            "    Saved features for 6650/9999 images in train/Dog...\n",
            "    Saved features for 6700/9999 images in train/Dog...\n",
            "    Saved features for 6750/9999 images in train/Dog...\n",
            "    Saved features for 6800/9999 images in train/Dog...\n",
            "    Saved features for 6850/9999 images in train/Dog...\n",
            "    Saved features for 6900/9999 images in train/Dog...\n",
            "    Saved features for 6950/9999 images in train/Dog...\n",
            "    Saved features for 7000/9999 images in train/Dog...\n",
            "    Saved features for 7050/9999 images in train/Dog...\n",
            "    Saved features for 7100/9999 images in train/Dog...\n",
            "    Saved features for 7150/9999 images in train/Dog...\n",
            "    Saved features for 7200/9999 images in train/Dog...\n",
            "    Saved features for 7250/9999 images in train/Dog...\n",
            "    Saved features for 7300/9999 images in train/Dog...\n",
            "    Saved features for 7350/9999 images in train/Dog...\n",
            "    Saved features for 7400/9999 images in train/Dog...\n",
            "    Saved features for 7450/9999 images in train/Dog...\n",
            "    Saved features for 7500/9999 images in train/Dog...\n",
            "    Saved features for 7550/9999 images in train/Dog...\n",
            "    Saved features for 7600/9999 images in train/Dog...\n",
            "    Saved features for 7650/9999 images in train/Dog...\n",
            "    Saved features for 7700/9999 images in train/Dog...\n",
            "    Saved features for 7750/9999 images in train/Dog...\n",
            "    Saved features for 7800/9999 images in train/Dog...\n",
            "    Saved features for 7850/9999 images in train/Dog...\n",
            "    Saved features for 7900/9999 images in train/Dog...\n",
            "    Saved features for 7950/9999 images in train/Dog...\n",
            "    Saved features for 8000/9999 images in train/Dog...\n",
            "    Saved features for 8050/9999 images in train/Dog...\n",
            "    Saved features for 8100/9999 images in train/Dog...\n",
            "    Saved features for 8150/9999 images in train/Dog...\n",
            "    Saved features for 8200/9999 images in train/Dog...\n",
            "    Saved features for 8250/9999 images in train/Dog...\n",
            "    Saved features for 8300/9999 images in train/Dog...\n",
            "    Saved features for 8350/9999 images in train/Dog...\n",
            "    Saved features for 8400/9999 images in train/Dog...\n",
            "    Saved features for 8450/9999 images in train/Dog...\n",
            "    Saved features for 8500/9999 images in train/Dog...\n",
            "    Saved features for 8550/9999 images in train/Dog...\n",
            "    Saved features for 8600/9999 images in train/Dog...\n",
            "    Saved features for 8650/9999 images in train/Dog...\n",
            "    Saved features for 8700/9999 images in train/Dog...\n",
            "    Saved features for 8750/9999 images in train/Dog...\n",
            "    Saved features for 8800/9999 images in train/Dog...\n",
            "    Saved features for 8850/9999 images in train/Dog...\n",
            "    Saved features for 8900/9999 images in train/Dog...\n",
            "    Saved features for 8950/9999 images in train/Dog...\n",
            "    Saved features for 9000/9999 images in train/Dog...\n",
            "    Saved features for 9050/9999 images in train/Dog...\n",
            "    Saved features for 9100/9999 images in train/Dog...\n",
            "    Saved features for 9150/9999 images in train/Dog...\n",
            "    Saved features for 9200/9999 images in train/Dog...\n",
            "    Saved features for 9250/9999 images in train/Dog...\n",
            "    Saved features for 9300/9999 images in train/Dog...\n",
            "    Saved features for 9350/9999 images in train/Dog...\n",
            "    Saved features for 9400/9999 images in train/Dog...\n",
            "    Saved features for 9450/9999 images in train/Dog...\n",
            "    Saved features for 9500/9999 images in train/Dog...\n",
            "    Saved features for 9550/9999 images in train/Dog...\n",
            "    Saved features for 9600/9999 images in train/Dog...\n",
            "    Saved features for 9650/9999 images in train/Dog...\n",
            "    Saved features for 9700/9999 images in train/Dog...\n",
            "    Saved features for 9750/9999 images in train/Dog...\n",
            "    Saved features for 9800/9999 images in train/Dog...\n",
            "    Saved features for 9850/9999 images in train/Dog...\n",
            "    Saved features for 9900/9999 images in train/Dog...\n",
            "    Saved features for 9950/9999 images in train/Dog...\n",
            "    Saved features for 9999/9999 images in train/Dog...\n",
            "  Finished processing class Dog. Extracted features for 9999 images.\n",
            "  Processing class: Cat\n",
            "    Saved features for 50/9999 images in train/Cat...\n",
            "    Saved features for 100/9999 images in train/Cat...\n",
            "    Saved features for 150/9999 images in train/Cat...\n",
            "    Saved features for 200/9999 images in train/Cat...\n",
            "    Saved features for 250/9999 images in train/Cat...\n",
            "    Saved features for 300/9999 images in train/Cat...\n",
            "    Saved features for 350/9999 images in train/Cat...\n",
            "    Saved features for 400/9999 images in train/Cat...\n",
            "    Saved features for 450/9999 images in train/Cat...\n",
            "    Saved features for 500/9999 images in train/Cat...\n",
            "    Saved features for 550/9999 images in train/Cat...\n",
            "    Saved features for 600/9999 images in train/Cat...\n",
            "    Saved features for 650/9999 images in train/Cat...\n",
            "    Saved features for 700/9999 images in train/Cat...\n",
            "    Saved features for 750/9999 images in train/Cat...\n",
            "    Saved features for 800/9999 images in train/Cat...\n",
            "    Saved features for 850/9999 images in train/Cat...\n",
            "    Saved features for 900/9999 images in train/Cat...\n",
            "    Saved features for 950/9999 images in train/Cat...\n",
            "    Saved features for 1000/9999 images in train/Cat...\n",
            "    Saved features for 1050/9999 images in train/Cat...\n",
            "    Saved features for 1100/9999 images in train/Cat...\n",
            "    Saved features for 1150/9999 images in train/Cat...\n",
            "    Saved features for 1200/9999 images in train/Cat...\n",
            "    Saved features for 1250/9999 images in train/Cat...\n",
            "    Saved features for 1300/9999 images in train/Cat...\n",
            "    Saved features for 1350/9999 images in train/Cat...\n",
            "    Saved features for 1400/9999 images in train/Cat...\n",
            "    Saved features for 1450/9999 images in train/Cat...\n",
            "    Saved features for 1500/9999 images in train/Cat...\n",
            "    Saved features for 1550/9999 images in train/Cat...\n",
            "    Saved features for 1600/9999 images in train/Cat...\n",
            "    Saved features for 1650/9999 images in train/Cat...\n",
            "    Saved features for 1700/9999 images in train/Cat...\n",
            "    Saved features for 1750/9999 images in train/Cat...\n",
            "    Saved features for 1800/9999 images in train/Cat...\n",
            "    Saved features for 1850/9999 images in train/Cat...\n",
            "    Saved features for 1900/9999 images in train/Cat...\n",
            "    Saved features for 1950/9999 images in train/Cat...\n",
            "    Saved features for 2000/9999 images in train/Cat...\n",
            "    Saved features for 2050/9999 images in train/Cat...\n",
            "    Saved features for 2100/9999 images in train/Cat...\n",
            "    Saved features for 2150/9999 images in train/Cat...\n",
            "    Saved features for 2200/9999 images in train/Cat...\n",
            "    Saved features for 2250/9999 images in train/Cat...\n",
            "    Saved features for 2300/9999 images in train/Cat...\n",
            "    Saved features for 2350/9999 images in train/Cat...\n",
            "    Saved features for 2400/9999 images in train/Cat...\n",
            "    Saved features for 2450/9999 images in train/Cat...\n",
            "    Saved features for 2500/9999 images in train/Cat...\n",
            "    Saved features for 2550/9999 images in train/Cat...\n",
            "    Saved features for 2600/9999 images in train/Cat...\n",
            "    Saved features for 2650/9999 images in train/Cat...\n",
            "    Saved features for 2700/9999 images in train/Cat...\n",
            "    Saved features for 2750/9999 images in train/Cat...\n",
            "    Saved features for 2800/9999 images in train/Cat...\n",
            "    Saved features for 2850/9999 images in train/Cat...\n",
            "    Saved features for 2900/9999 images in train/Cat...\n",
            "    Saved features for 2950/9999 images in train/Cat...\n",
            "    Saved features for 3000/9999 images in train/Cat...\n",
            "    Saved features for 3050/9999 images in train/Cat...\n",
            "    Saved features for 3100/9999 images in train/Cat...\n",
            "    Saved features for 3150/9999 images in train/Cat...\n",
            "    Saved features for 3200/9999 images in train/Cat...\n",
            "    Saved features for 3250/9999 images in train/Cat...\n",
            "    Saved features for 3300/9999 images in train/Cat...\n",
            "    Saved features for 3350/9999 images in train/Cat...\n",
            "    Saved features for 3400/9999 images in train/Cat...\n",
            "    Saved features for 3450/9999 images in train/Cat...\n",
            "    Saved features for 3500/9999 images in train/Cat...\n",
            "    Saved features for 3550/9999 images in train/Cat...\n",
            "    Saved features for 3600/9999 images in train/Cat...\n",
            "    Saved features for 3650/9999 images in train/Cat...\n",
            "    Saved features for 3700/9999 images in train/Cat...\n",
            "    Saved features for 3750/9999 images in train/Cat...\n",
            "    Saved features for 3800/9999 images in train/Cat...\n",
            "    Saved features for 3850/9999 images in train/Cat...\n",
            "    Saved features for 3900/9999 images in train/Cat...\n",
            "    Saved features for 3950/9999 images in train/Cat...\n",
            "    Saved features for 4000/9999 images in train/Cat...\n",
            "    Saved features for 4050/9999 images in train/Cat...\n",
            "    Saved features for 4100/9999 images in train/Cat...\n",
            "    Saved features for 4150/9999 images in train/Cat...\n",
            "    Saved features for 4200/9999 images in train/Cat...\n",
            "    Saved features for 4250/9999 images in train/Cat...\n",
            "    Saved features for 4300/9999 images in train/Cat...\n",
            "    Saved features for 4350/9999 images in train/Cat...\n",
            "    Saved features for 4400/9999 images in train/Cat...\n",
            "    Saved features for 4450/9999 images in train/Cat...\n",
            "    Saved features for 4500/9999 images in train/Cat...\n",
            "    Saved features for 4550/9999 images in train/Cat...\n",
            "    Saved features for 4600/9999 images in train/Cat...\n",
            "    Saved features for 4650/9999 images in train/Cat...\n",
            "    Saved features for 4700/9999 images in train/Cat...\n",
            "    Saved features for 4750/9999 images in train/Cat...\n",
            "    Saved features for 4800/9999 images in train/Cat...\n",
            "    Saved features for 4850/9999 images in train/Cat...\n",
            "    Saved features for 4900/9999 images in train/Cat...\n",
            "    Saved features for 4950/9999 images in train/Cat...\n",
            "    Saved features for 5000/9999 images in train/Cat...\n",
            "    Saved features for 5050/9999 images in train/Cat...\n",
            "    Saved features for 5100/9999 images in train/Cat...\n",
            "    Saved features for 5150/9999 images in train/Cat...\n",
            "    Saved features for 5200/9999 images in train/Cat...\n",
            "    Saved features for 5250/9999 images in train/Cat...\n",
            "    Saved features for 5300/9999 images in train/Cat...\n",
            "    Saved features for 5350/9999 images in train/Cat...\n",
            "    Saved features for 5400/9999 images in train/Cat...\n",
            "    Saved features for 5450/9999 images in train/Cat...\n",
            "    Saved features for 5500/9999 images in train/Cat...\n",
            "    Saved features for 5550/9999 images in train/Cat...\n",
            "    Saved features for 5600/9999 images in train/Cat...\n",
            "    Saved features for 5650/9999 images in train/Cat...\n",
            "    Saved features for 5700/9999 images in train/Cat...\n",
            "    Saved features for 5750/9999 images in train/Cat...\n",
            "    Saved features for 5800/9999 images in train/Cat...\n",
            "    Saved features for 5850/9999 images in train/Cat...\n",
            "    Saved features for 5900/9999 images in train/Cat...\n",
            "    Saved features for 5950/9999 images in train/Cat...\n",
            "    Saved features for 6000/9999 images in train/Cat...\n",
            "    Saved features for 6050/9999 images in train/Cat...\n",
            "    Saved features for 6100/9999 images in train/Cat...\n",
            "    Saved features for 6150/9999 images in train/Cat...\n",
            "    Saved features for 6200/9999 images in train/Cat...\n",
            "    Saved features for 6250/9999 images in train/Cat...\n",
            "    Saved features for 6300/9999 images in train/Cat...\n",
            "    Saved features for 6350/9999 images in train/Cat...\n",
            "    Saved features for 6400/9999 images in train/Cat...\n",
            "    Saved features for 6450/9999 images in train/Cat...\n",
            "    Saved features for 6500/9999 images in train/Cat...\n",
            "    Saved features for 6550/9999 images in train/Cat...\n",
            "    Saved features for 6600/9999 images in train/Cat...\n",
            "    Saved features for 6650/9999 images in train/Cat...\n",
            "    Saved features for 6700/9999 images in train/Cat...\n",
            "    Saved features for 6750/9999 images in train/Cat...\n",
            "    Saved features for 6800/9999 images in train/Cat...\n",
            "    Saved features for 6850/9999 images in train/Cat...\n",
            "    Saved features for 6900/9999 images in train/Cat...\n",
            "    Saved features for 6950/9999 images in train/Cat...\n",
            "    Saved features for 7000/9999 images in train/Cat...\n",
            "    Saved features for 7050/9999 images in train/Cat...\n",
            "    Saved features for 7100/9999 images in train/Cat...\n",
            "    Saved features for 7150/9999 images in train/Cat...\n",
            "    Saved features for 7200/9999 images in train/Cat...\n",
            "    Saved features for 7250/9999 images in train/Cat...\n",
            "    Saved features for 7300/9999 images in train/Cat...\n",
            "    Saved features for 7350/9999 images in train/Cat...\n",
            "    Saved features for 7400/9999 images in train/Cat...\n",
            "    Saved features for 7450/9999 images in train/Cat...\n",
            "    Saved features for 7500/9999 images in train/Cat...\n",
            "    Saved features for 7550/9999 images in train/Cat...\n",
            "    Saved features for 7600/9999 images in train/Cat...\n",
            "    Saved features for 7650/9999 images in train/Cat...\n",
            "    Saved features for 7700/9999 images in train/Cat...\n",
            "    Saved features for 7750/9999 images in train/Cat...\n",
            "    Saved features for 7800/9999 images in train/Cat...\n",
            "    Saved features for 7850/9999 images in train/Cat...\n",
            "    Saved features for 7900/9999 images in train/Cat...\n",
            "    Saved features for 7950/9999 images in train/Cat...\n",
            "    Saved features for 8000/9999 images in train/Cat...\n",
            "    Saved features for 8050/9999 images in train/Cat...\n",
            "    Saved features for 8100/9999 images in train/Cat...\n",
            "    Saved features for 8150/9999 images in train/Cat...\n",
            "    Saved features for 8200/9999 images in train/Cat...\n",
            "    Saved features for 8250/9999 images in train/Cat...\n",
            "    Saved features for 8300/9999 images in train/Cat...\n",
            "    Saved features for 8350/9999 images in train/Cat...\n",
            "    Saved features for 8400/9999 images in train/Cat...\n",
            "    Saved features for 8450/9999 images in train/Cat...\n",
            "    Saved features for 8500/9999 images in train/Cat...\n",
            "    Saved features for 8550/9999 images in train/Cat...\n",
            "    Saved features for 8600/9999 images in train/Cat...\n",
            "    Saved features for 8650/9999 images in train/Cat...\n",
            "    Saved features for 8700/9999 images in train/Cat...\n",
            "    Saved features for 8750/9999 images in train/Cat...\n",
            "    Saved features for 8800/9999 images in train/Cat...\n",
            "    Saved features for 8850/9999 images in train/Cat...\n",
            "    Saved features for 8900/9999 images in train/Cat...\n",
            "    Saved features for 8950/9999 images in train/Cat...\n",
            "    Saved features for 9000/9999 images in train/Cat...\n",
            "    Saved features for 9050/9999 images in train/Cat...\n",
            "    Saved features for 9100/9999 images in train/Cat...\n",
            "    Saved features for 9150/9999 images in train/Cat...\n",
            "    Saved features for 9200/9999 images in train/Cat...\n",
            "    Saved features for 9250/9999 images in train/Cat...\n",
            "    Saved features for 9300/9999 images in train/Cat...\n",
            "    Saved features for 9350/9999 images in train/Cat...\n",
            "    Saved features for 9400/9999 images in train/Cat...\n",
            "    Saved features for 9450/9999 images in train/Cat...\n",
            "    Saved features for 9500/9999 images in train/Cat...\n",
            "    Saved features for 9550/9999 images in train/Cat...\n",
            "    Saved features for 9600/9999 images in train/Cat...\n",
            "    Saved features for 9650/9999 images in train/Cat...\n",
            "    Saved features for 9700/9999 images in train/Cat...\n",
            "    Saved features for 9750/9999 images in train/Cat...\n",
            "    Saved features for 9800/9999 images in train/Cat...\n",
            "    Saved features for 9850/9999 images in train/Cat...\n",
            "    Saved features for 9900/9999 images in train/Cat...\n",
            "    Saved features for 9950/9999 images in train/Cat...\n",
            "    Saved features for 9999/9999 images in train/Cat...\n",
            "  Finished processing class Cat. Extracted features for 9999 images.\n",
            "Finished processing split train.\n",
            "\n",
            "All feature extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/feature_vectors.zip /kaggle/feature_vectors/\n",
        "from google.colab import files\n",
        "files.download(\"/content/feature_vectors.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UW1jtZZtI-JX",
        "outputId": "c8773407-9a5a-4de4-fe36-31cf11b7035f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e85bbb05-40f7-46ef-9e1d-ca92036e3f0f\", \"feature_vectors.zip\", 110928842)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# --- Imports ---\n",
        "# ------------------------------------------------------------\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader # For data loading and batching\n",
        "from sklearn.model_selection import train_test_split # Still useful if you needed to split data here\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# --- Configuration ---\n",
        "# ------------------------------------------------------------\n",
        "# Path to the directory where you saved the extracted feature vectors\n",
        "# !!! CHANGE THIS TO YOUR ACTUAL SAVED FEATURES PATH !!!\n",
        "SAVED_FEATURES_BASE_DIR = '/kaggle/feature_vectors'\n",
        "\n",
        "# Label mapping (must match how features were extracted)\n",
        "# Ensure these map to 0 and 1 for binary classification\n",
        "LABEL_MAP = {'Cat': 0, 'Dog': 1} # Cat = 0, Dog = 1\n",
        "\n",
        "# --- Classifier Model Parameters ---\n",
        "INPUT_FEATURE_DIM = 4096 # VGG16 4096 feature vector size\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "EPOCHS = 50 # Number of training epochs (you might adjust this)\n",
        "BATCH_SIZE = 32 # Training batch size (you might adjust this)\n",
        "LEARNING_RATE = 0.001 # Learning rate for optimizer (Adam is a good default)\n",
        "\n",
        "# --- Device Configuration ---\n",
        "# Use GPU if available, otherwise use CPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# --- Data Loading Function (from .npy files) ---\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def load_features_and_labels_from_dir(features_base_dir, split_name, label_map):\n",
        "    \"\"\"\n",
        "    Loads feature vectors and labels from .npy files saved in a directory structure.\n",
        "\n",
        "    Args:\n",
        "        features_base_dir: The base directory containing the split folders (e.g., '/kaggle/feature_vectors').\n",
        "        split_name: The name of the split folder to load ('train', 'val', or 'test').\n",
        "        label_map: Dictionary mapping class names (folder names) to numerical labels.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - features: A NumPy array of shape (num_samples, INPUT_FEATURE_DIM).\n",
        "        - labels: A NumPy array of shape (num_samples,) containing numerical labels.\n",
        "    \"\"\"\n",
        "    split_dir = os.path.join(features_base_dir, split_name)\n",
        "    if not os.path.exists(split_dir):\n",
        "        raise FileNotFoundError(f\"Split directory not found: {split_dir}\")\n",
        "\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Loading features from split: {split_dir}\")\n",
        "\n",
        "    # Iterate through class folders ('Cat', 'Dog') based on the provided label_map\n",
        "    for class_name, label in label_map.items():\n",
        "        class_dir = os.path.join(split_dir, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            print(f\"  Warning: Class directory not found: {class_dir}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  Loading features for class: {class_name}\")\n",
        "        npy_files = [f for f in os.listdir(class_dir) if f.lower().endswith('.npy')]\n",
        "\n",
        "        loaded_count = 0\n",
        "        for npy_file in npy_files:\n",
        "            file_path = os.path.join(class_dir, npy_file)\n",
        "            try:\n",
        "                # Load the .npy file\n",
        "                feature_vector = np.load(file_path)\n",
        "\n",
        "                # Ensure the shape is correct (should be (4096,))\n",
        "                if feature_vector.shape != (INPUT_FEATURE_DIM,):\n",
        "                    print(f\"    Warning: Unexpected shape {feature_vector.shape} for {npy_file}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                all_features.append(feature_vector)\n",
        "                all_labels.append(label) # Append the numerical label\n",
        "\n",
        "                loaded_count += 1\n",
        "                if loaded_count > 0 and loaded_count % 1000 == 0: # Print progress\n",
        "                     print(f\"    Loaded {loaded_count}/{len(npy_files)} files for {class_name}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    Error loading {npy_file}: {e}. Skipping.\")\n",
        "\n",
        "        print(f\"  Finished loading {loaded_count} files for class {class_name}.\")\n",
        "\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    features = np.array(all_features)\n",
        "    labels = np.array(all_labels)\n",
        "\n",
        "    print(f\"Finished loading split {split_name}.\")\n",
        "    print(f\"  Loaded {len(features)} total samples.\")\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading feature vectors and labels.\"\"\"\n",
        "    def __init__(self, features_np, labels_np):\n",
        "        self.features = torch.from_numpy(features_np).float()\n",
        "        self.labels = torch.from_numpy(labels_np).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "class FeatureClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        # Simple feed-forward network (MLP)\n",
        "        self.layer_1 = nn.Linear(input_dim, 256) # Input layer to hidden layer 1\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dropout_1 = nn.Dropout(0.5) # Dropout for regularization\n",
        "\n",
        "        # Optional: Add another hidden layer if needed\n",
        "        # self.layer_2 = nn.Linear(256, 128)\n",
        "        # self.relu_2 = nn.ReLU()\n",
        "        # self.dropout_2 = nn.Dropout(0.5)\n",
        "\n",
        "        # Output layer\n",
        "        # Output size is num_classes (2 for Cat/Dog)\n",
        "        self.output_layer = nn.Linear(256, num_classes) # Connect last hidden layer to output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.dropout_1(x)\n",
        "\n",
        "        # If you added layer_2:\n",
        "        # x = self.layer_2(x)\n",
        "        # x = self.relu_2(x)\n",
        "        # x = self.dropout_2(x)\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.train() # Set model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        train_total = 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # Move data to device\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track training statistics\n",
        "            running_loss += loss.item() * inputs.size(0) # Accumulate loss\n",
        "            _, predicted = torch.max(outputs.data, 1)     # Get predicted class\n",
        "            train_total += labels.size(0)                 # Count total samples\n",
        "            train_corrects += (predicted == labels).sum().item() # Count correct predictions\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = train_corrects / train_total\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Set model back to training mode for the next epoch\n",
        "        model.train()\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculation for evaluation\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # Move data to device\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            corrects += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader.dataset)\n",
        "    epoch_acc = corrects / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# --- Main Execution ---\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n--- PyTorch Classifier Training ---\")\n",
        "\n",
        "    # 1. Load Data from .npy files\n",
        "    try:\n",
        "        X_train_np, y_train_np = load_features_and_labels_from_dir(SAVED_FEATURES_BASE_DIR, 'train', LABEL_MAP)\n",
        "        X_val_np, y_val_np = load_features_and_labels_from_dir(SAVED_FEATURES_BASE_DIR, 'val', LABEL_MAP)\n",
        "        X_test_np, y_test_np = load_features_and_labels_from_dir(SAVED_FEATURES_BASE_DIR, 'test', LABEL_MAP)\n",
        "    except FileNotFoundError as e:\n",
        "         print(f\"Data loading failed: {e}\")\n",
        "         print(\"Please ensure SAVED_FEATURES_BASE_DIR is correct and .npy files exist.\")\n",
        "         exit() # Exit if data cannot be loaded\n",
        "\n",
        "    if len(X_train_np) == 0:\n",
        "        print(\"\\nERROR: No training features loaded. Cannot proceed.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    # 2. Create Datasets and DataLoaders\n",
        "    print(\"\\nCreating Datasets and DataLoaders...\")\n",
        "    train_dataset = FeatureDataset(X_train_np, y_train_np)\n",
        "    val_dataset = FeatureDataset(X_val_np, y_val_np)\n",
        "    test_dataset = FeatureDataset(X_test_np, y_test_np)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) # Shuffle training data\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(\"Datasets and DataLoaders created.\")\n",
        "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
        "\n",
        "\n",
        "    # 3. Define the PyTorch Classifier Model\n",
        "    print(\"\\nDefining PyTorch Classifier Model...\")\n",
        "    model = FeatureClassifier(INPUT_FEATURE_DIM, NUM_CLASSES)\n",
        "    model.to(DEVICE) # Move model to the configured device (CPU/GPU)\n",
        "    print(\"Model Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "\n",
        "    # 4. Define Loss Function and Optimizer\n",
        "    print(\"\\nDefining Loss and Optimizer...\")\n",
        "    criterion = nn.CrossEntropyLoss() # Suitable for classification with raw logits\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) # Adam optimizer\n",
        "\n",
        "\n",
        "    # 5. Train the Model\n",
        "    print(\"\\n--- Training Classifier Model ---\")\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, DEVICE)\n",
        "\n",
        "\n",
        "    # 6. Evaluate the Model on Test Set\n",
        "    print(\"\\n--- Evaluating Classifier Model on Test Set ---\")\n",
        "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, DEVICE)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    print(\"\\nClassifier training and evaluation finished.\")\n",
        "    print(\"The trained PyTorch model is stored in the 'model' variable.\")\n",
        "    # You can save the trained model weights: torch.save(model.state_dict(), 'classifier_weights.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE3GLlfpJngB",
        "outputId": "bcfc49a0-5550-435c-8455-412143a5b33f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "--- PyTorch Classifier Training ---\n",
            "Loading features from split: /kaggle/feature_vectors/train\n",
            "  Loading features for class: Cat\n",
            "    Loaded 1000/9999 files for Cat...\n",
            "    Loaded 2000/9999 files for Cat...\n",
            "    Loaded 3000/9999 files for Cat...\n",
            "    Loaded 4000/9999 files for Cat...\n",
            "    Loaded 5000/9999 files for Cat...\n",
            "    Loaded 6000/9999 files for Cat...\n",
            "    Loaded 7000/9999 files for Cat...\n",
            "    Loaded 8000/9999 files for Cat...\n",
            "    Loaded 9000/9999 files for Cat...\n",
            "  Finished loading 9999 files for class Cat.\n",
            "  Loading features for class: Dog\n",
            "    Loaded 1000/9999 files for Dog...\n",
            "    Loaded 2000/9999 files for Dog...\n",
            "    Loaded 3000/9999 files for Dog...\n",
            "    Loaded 4000/9999 files for Dog...\n",
            "    Loaded 5000/9999 files for Dog...\n",
            "    Loaded 6000/9999 files for Dog...\n",
            "    Loaded 7000/9999 files for Dog...\n",
            "    Loaded 8000/9999 files for Dog...\n",
            "    Loaded 9000/9999 files for Dog...\n",
            "  Finished loading 9999 files for class Dog.\n",
            "Finished loading split train.\n",
            "  Loaded 19998 total samples.\n",
            "Loading features from split: /kaggle/feature_vectors/val\n",
            "  Loading features for class: Cat\n",
            "    Loaded 1000/1250 files for Cat...\n",
            "  Finished loading 1250 files for class Cat.\n",
            "  Loading features for class: Dog\n",
            "    Loaded 1000/1250 files for Dog...\n",
            "  Finished loading 1250 files for class Dog.\n",
            "Finished loading split val.\n",
            "  Loaded 2500 total samples.\n",
            "Loading features from split: /kaggle/feature_vectors/test\n",
            "  Loading features for class: Cat\n",
            "    Loaded 1000/1250 files for Cat...\n",
            "  Finished loading 1250 files for class Cat.\n",
            "  Loading features for class: Dog\n",
            "    Loaded 1000/1250 files for Dog...\n",
            "  Finished loading 1250 files for class Dog.\n",
            "Finished loading split test.\n",
            "  Loaded 2500 total samples.\n",
            "\n",
            "Creating Datasets and DataLoaders...\n",
            "Datasets and DataLoaders created.\n",
            "Train batches: 625, Val batches: 79, Test batches: 79\n",
            "\n",
            "Defining PyTorch Classifier Model...\n",
            "Model Architecture:\n",
            "FeatureClassifier(\n",
            "  (layer_1): Linear(in_features=4096, out_features=256, bias=True)\n",
            "  (relu_1): ReLU()\n",
            "  (dropout_1): Dropout(p=0.5, inplace=False)\n",
            "  (output_layer): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Defining Loss and Optimizer...\n",
            "\n",
            "--- Training Classifier Model ---\n",
            "Epoch 1/50 - Train Loss: 0.0577, Train Acc: 0.9817 - Val Loss: 0.0329, Val Acc: 0.9900\n",
            "Epoch 2/50 - Train Loss: 0.0302, Train Acc: 0.9903 - Val Loss: 0.0350, Val Acc: 0.9888\n",
            "Epoch 3/50 - Train Loss: 0.0260, Train Acc: 0.9910 - Val Loss: 0.0435, Val Acc: 0.9860\n",
            "Epoch 4/50 - Train Loss: 0.0204, Train Acc: 0.9938 - Val Loss: 0.0368, Val Acc: 0.9888\n",
            "Epoch 5/50 - Train Loss: 0.0166, Train Acc: 0.9946 - Val Loss: 0.0425, Val Acc: 0.9884\n",
            "Epoch 6/50 - Train Loss: 0.0167, Train Acc: 0.9953 - Val Loss: 0.0442, Val Acc: 0.9908\n",
            "Epoch 7/50 - Train Loss: 0.0110, Train Acc: 0.9960 - Val Loss: 0.0676, Val Acc: 0.9872\n",
            "Epoch 8/50 - Train Loss: 0.0127, Train Acc: 0.9961 - Val Loss: 0.0643, Val Acc: 0.9920\n",
            "Epoch 9/50 - Train Loss: 0.0157, Train Acc: 0.9953 - Val Loss: 0.0618, Val Acc: 0.9896\n",
            "Epoch 10/50 - Train Loss: 0.0140, Train Acc: 0.9965 - Val Loss: 0.0699, Val Acc: 0.9884\n",
            "Epoch 11/50 - Train Loss: 0.0131, Train Acc: 0.9963 - Val Loss: 0.0675, Val Acc: 0.9892\n",
            "Epoch 12/50 - Train Loss: 0.0076, Train Acc: 0.9974 - Val Loss: 0.0726, Val Acc: 0.9924\n",
            "Epoch 13/50 - Train Loss: 0.0090, Train Acc: 0.9982 - Val Loss: 0.0713, Val Acc: 0.9924\n",
            "Epoch 14/50 - Train Loss: 0.0071, Train Acc: 0.9980 - Val Loss: 0.0729, Val Acc: 0.9904\n",
            "Epoch 15/50 - Train Loss: 0.0060, Train Acc: 0.9984 - Val Loss: 0.0760, Val Acc: 0.9908\n",
            "Epoch 16/50 - Train Loss: 0.0074, Train Acc: 0.9981 - Val Loss: 0.0876, Val Acc: 0.9884\n",
            "Epoch 17/50 - Train Loss: 0.0077, Train Acc: 0.9979 - Val Loss: 0.1005, Val Acc: 0.9904\n",
            "Epoch 18/50 - Train Loss: 0.0066, Train Acc: 0.9983 - Val Loss: 0.1037, Val Acc: 0.9888\n",
            "Epoch 19/50 - Train Loss: 0.0111, Train Acc: 0.9974 - Val Loss: 0.0950, Val Acc: 0.9876\n",
            "Epoch 20/50 - Train Loss: 0.0055, Train Acc: 0.9982 - Val Loss: 0.0915, Val Acc: 0.9904\n",
            "Epoch 21/50 - Train Loss: 0.0075, Train Acc: 0.9985 - Val Loss: 0.1526, Val Acc: 0.9840\n",
            "Epoch 22/50 - Train Loss: 0.0087, Train Acc: 0.9978 - Val Loss: 0.0868, Val Acc: 0.9912\n",
            "Epoch 23/50 - Train Loss: 0.0041, Train Acc: 0.9987 - Val Loss: 0.1087, Val Acc: 0.9892\n",
            "Epoch 24/50 - Train Loss: 0.0074, Train Acc: 0.9984 - Val Loss: 0.1269, Val Acc: 0.9888\n",
            "Epoch 25/50 - Train Loss: 0.0111, Train Acc: 0.9977 - Val Loss: 0.0967, Val Acc: 0.9900\n",
            "Epoch 26/50 - Train Loss: 0.0094, Train Acc: 0.9980 - Val Loss: 0.1232, Val Acc: 0.9908\n",
            "Epoch 27/50 - Train Loss: 0.0040, Train Acc: 0.9993 - Val Loss: 0.0999, Val Acc: 0.9916\n",
            "Epoch 28/50 - Train Loss: 0.0051, Train Acc: 0.9988 - Val Loss: 0.1178, Val Acc: 0.9904\n",
            "Epoch 29/50 - Train Loss: 0.0076, Train Acc: 0.9988 - Val Loss: 0.0977, Val Acc: 0.9916\n",
            "Epoch 30/50 - Train Loss: 0.0025, Train Acc: 0.9991 - Val Loss: 0.1086, Val Acc: 0.9900\n",
            "Epoch 31/50 - Train Loss: 0.0106, Train Acc: 0.9982 - Val Loss: 0.1205, Val Acc: 0.9888\n",
            "Epoch 32/50 - Train Loss: 0.0075, Train Acc: 0.9984 - Val Loss: 0.1368, Val Acc: 0.9892\n",
            "Epoch 33/50 - Train Loss: 0.0076, Train Acc: 0.9985 - Val Loss: 0.1075, Val Acc: 0.9896\n",
            "Epoch 34/50 - Train Loss: 0.0064, Train Acc: 0.9989 - Val Loss: 0.1174, Val Acc: 0.9908\n",
            "Epoch 35/50 - Train Loss: 0.0054, Train Acc: 0.9989 - Val Loss: 0.1242, Val Acc: 0.9908\n",
            "Epoch 36/50 - Train Loss: 0.0038, Train Acc: 0.9990 - Val Loss: 0.1331, Val Acc: 0.9900\n",
            "Epoch 37/50 - Train Loss: 0.0075, Train Acc: 0.9985 - Val Loss: 0.1751, Val Acc: 0.9868\n",
            "Epoch 38/50 - Train Loss: 0.0059, Train Acc: 0.9987 - Val Loss: 0.1916, Val Acc: 0.9868\n",
            "Epoch 39/50 - Train Loss: 0.0057, Train Acc: 0.9989 - Val Loss: 0.1491, Val Acc: 0.9908\n",
            "Epoch 40/50 - Train Loss: 0.0037, Train Acc: 0.9991 - Val Loss: 0.1402, Val Acc: 0.9912\n",
            "Epoch 41/50 - Train Loss: 0.0025, Train Acc: 0.9993 - Val Loss: 0.1698, Val Acc: 0.9904\n",
            "Epoch 42/50 - Train Loss: 0.0102, Train Acc: 0.9985 - Val Loss: 0.2237, Val Acc: 0.9896\n",
            "Epoch 43/50 - Train Loss: 0.0057, Train Acc: 0.9989 - Val Loss: 0.1967, Val Acc: 0.9904\n",
            "Epoch 44/50 - Train Loss: 0.0071, Train Acc: 0.9987 - Val Loss: 0.2130, Val Acc: 0.9908\n",
            "Epoch 45/50 - Train Loss: 0.0096, Train Acc: 0.9985 - Val Loss: 0.2584, Val Acc: 0.9904\n",
            "Epoch 46/50 - Train Loss: 0.0072, Train Acc: 0.9988 - Val Loss: 0.2105, Val Acc: 0.9876\n",
            "Epoch 47/50 - Train Loss: 0.0063, Train Acc: 0.9993 - Val Loss: 0.1657, Val Acc: 0.9900\n",
            "Epoch 48/50 - Train Loss: 0.0042, Train Acc: 0.9993 - Val Loss: 0.1955, Val Acc: 0.9888\n",
            "Epoch 49/50 - Train Loss: 0.0096, Train Acc: 0.9991 - Val Loss: 0.2117, Val Acc: 0.9900\n",
            "Epoch 50/50 - Train Loss: 0.0049, Train Acc: 0.9995 - Val Loss: 0.1877, Val Acc: 0.9916\n",
            "\n",
            "--- Evaluating Classifier Model on Test Set ---\n",
            "Test Loss: 0.0985\n",
            "Test Accuracy: 0.9912\n",
            "\n",
            "Classifier training and evaluation finished.\n",
            "The trained PyTorch model is stored in the 'model' variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lURT96b5l_t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3i0CBKXGpnV",
        "outputId": "b289ca14-8544-443c-8875-08ee0bbb6deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19998 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "input_size = (128, 128)\n",
        "batch_size = 128\n",
        "\n",
        "augmentor = ImageDataGenerator(samplewise_center=True,\n",
        "                               samplewise_std_normalization=True)\n",
        "\n",
        "train_generator = augmentor.flow_from_directory(\n",
        "    '/kaggle/split_data/train',\n",
        "    target_size=input_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = augmentor.flow_from_directory(\n",
        "    '/kaggle/split_data/val',\n",
        "    target_size=input_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = augmentor.flow_from_directory(\n",
        "    '/kaggle/split_data/test',\n",
        "    target_size=input_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WeZtKXpdG1wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8618a9-4a08-4847-d9b8-507ed45bd011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(train_generator.class_indices)\n",
        "input_shape = (128,128,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn7-xooXG2nV",
        "outputId": "d6c00d3f-cd7e-4df3-938b-10438d5815f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Layers:\n",
            "0 conv2d N/A\n",
            "1 conv2d_1 N/A\n",
            "2 max_pooling2d N/A\n",
            "3 dropout N/A\n",
            "4 conv2d_2 N/A\n",
            "5 conv2d_3 N/A\n",
            "6 max_pooling2d_1 N/A\n",
            "7 dropout_1 N/A\n",
            "8 conv2d_4 N/A\n",
            "9 conv2d_5 N/A\n",
            "10 max_pooling2d_2 N/A\n",
            "11 dropout_2 N/A\n",
            "12 conv2d_6 N/A\n",
            "13 conv2d_7 N/A\n",
            "14 final_pool_layer N/A\n",
            "15 dropout_3 N/A\n",
            "16 flatten N/A\n",
            "17 dense N/A\n",
            "18 dense_1 N/A\n",
            "19 dropout_4 N/A\n",
            "20 dense_2 N/A\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Input(shape = input_shape),\n",
        "    Conv2D (32, kernel_size = (3,3), activation= \"relu\"),\n",
        "    Conv2D (32, kernel_size = (3,3), activation= \"relu\"),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "\n",
        "    Conv2D (64, kernel_size = (3,3), activation= \"relu\"),\n",
        "    Conv2D (64, kernel_size = (3,3), activation= \"relu\"),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "\n",
        "    Conv2D (128, kernel_size = (3,3), activation= \"relu\"),\n",
        "    Conv2D (128, kernel_size = (3,3), activation= \"relu\"),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D (256, kernel_size = (3,3), activation= \"relu\"),\n",
        "    Conv2D (256, kernel_size = (3,3), activation= \"relu\"),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation = 'relu'),\n",
        "    Dense(128 , activation = 'relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "             loss= 'categorical_crossentropy',\n",
        "             metrics =[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARehXgtnG8xo",
        "outputId": "03512a2e-6902-4cc2-e546-219d7a90149d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m  6/157\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 168ms/step - accuracy: 0.4882 - loss: 0.7747"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.5164 - loss: 0.6986"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 316ms/step - accuracy: 0.5167 - loss: 0.6984 - val_accuracy: 0.6452 - val_loss: 0.6299\n",
            "Epoch 2/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - accuracy: 0.6764 - loss: 0.5975 - val_accuracy: 0.6960 - val_loss: 0.5981\n",
            "Epoch 3/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 231ms/step - accuracy: 0.7258 - loss: 0.5476 - val_accuracy: 0.7588 - val_loss: 0.5115\n",
            "Epoch 4/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - accuracy: 0.7601 - loss: 0.4934 - val_accuracy: 0.7696 - val_loss: 0.4738\n",
            "Epoch 5/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 227ms/step - accuracy: 0.7958 - loss: 0.4378 - val_accuracy: 0.8096 - val_loss: 0.4096\n",
            "Epoch 6/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 227ms/step - accuracy: 0.8291 - loss: 0.3863 - val_accuracy: 0.8372 - val_loss: 0.3638\n",
            "Epoch 7/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 231ms/step - accuracy: 0.8541 - loss: 0.3394 - val_accuracy: 0.8708 - val_loss: 0.3125\n",
            "Epoch 8/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 231ms/step - accuracy: 0.8712 - loss: 0.2992 - val_accuracy: 0.8672 - val_loss: 0.3055\n",
            "Epoch 9/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 226ms/step - accuracy: 0.8834 - loss: 0.2769 - val_accuracy: 0.8880 - val_loss: 0.2668\n",
            "Epoch 10/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - accuracy: 0.8941 - loss: 0.2526 - val_accuracy: 0.9016 - val_loss: 0.2490\n",
            "Epoch 11/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 229ms/step - accuracy: 0.9045 - loss: 0.2273 - val_accuracy: 0.9008 - val_loss: 0.2287\n",
            "Epoch 12/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 229ms/step - accuracy: 0.9157 - loss: 0.2030 - val_accuracy: 0.8964 - val_loss: 0.2837\n",
            "Epoch 13/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - accuracy: 0.9225 - loss: 0.1916 - val_accuracy: 0.9028 - val_loss: 0.2409\n",
            "Epoch 14/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 230ms/step - accuracy: 0.9255 - loss: 0.1817 - val_accuracy: 0.9044 - val_loss: 0.2322\n",
            "Epoch 15/15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 229ms/step - accuracy: 0.9266 - loss: 0.1702 - val_accuracy: 0.8964 - val_loss: 0.2625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f287069d210>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "model.fit(train_generator,\n",
        "          epochs=15,\n",
        "          validation_data=val_generator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2J2kSKDcUQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Try accessing the model's input tensor if build worked correctly\n",
        "    # (This is what *should* work)\n",
        "    model_input_tensor = model.input\n",
        "    if model_input_tensor is None: # Check if it's None\n",
        "         raise AttributeError # Force fallback if None\n",
        "    print(\"Using model.input\")\n",
        "except AttributeError:\n",
        "    try:\n",
        "        # Fallback: If Input() is the first layer *in the list*:\n",
        "        model_input_tensor = model.layers[0].output\n",
        "        print(\"Using model.layers[0].output\")\n",
        "         # This might actually get the output of the *first Conv layer* if Input isn't counted\n",
        "         # Need to be careful. Let's try model.inputs list\n",
        "        if isinstance(model.inputs, list) and len(model.inputs) > 0:\n",
        "             model_input_tensor = model.inputs[0]\n",
        "             print(\"Using model.inputs[0]\")\n",
        "        else:\n",
        "             # If the above fail, maybe the input layer NAME works?\n",
        "             model_input_tensor = model.get_layer(index=0).output # Try getting layer 0 output by index\n",
        "             print(\"Using model.get_layer(index=0).output\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting input tensor: {e}\")\n",
        "        print(\"Could not reliably determine the input tensor. Consider defining Input() outside the Sequential list.\")\n",
        "        # Handle error appropriately - maybe exit or raise\n",
        "        raise ValueError(\"Cannot find the input tensor for the feature extractor.\") from e\n",
        "\n",
        "\n",
        "# Identify the target layer by name (more robust)\n",
        "target_layer_name = 'final_pool_layer'\n",
        "target_output = model.get_layer(target_layer_name).output\n",
        "\n",
        "# Create a new Model using the input tensor we found\n",
        "feature_extractor_model = Model(inputs=model_input_tensor, outputs=target_output, name=\"feature_extractor\")\n",
        "\n",
        "print(\"\\n--- Feature Extractor Model Summary ---\")\n",
        "# Need to build the extractor model too before summary sometimes\n",
        "feature_extractor_model.build(input_shape=(None,) + input_shape)\n",
        "feature_extractor_model.summary()\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_shape):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_shape[:2])\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0\n",
        "    return img_array\n",
        "\n",
        "image_path = '/kaggle/split_data/test/Cat/100.jpg'\n",
        "try:\n",
        "    # Create a dummy image if needed for testing:\n",
        "    dummy_data = np.random.rand(input_shape[0], input_shape[1], input_shape[2]) * 255\n",
        "    tf.keras.preprocessing.image.save_img(image_path, dummy_data)\n",
        "except Exception as e:\n",
        "    print(f\"Could not create dummy image: {e}. Please provide a real image path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "preprocessed_image = load_and_preprocess_image(image_path, input_shape)\n",
        "\n",
        "# Use the feature_extractor_model to get the feature map\n",
        "feature_map = feature_extractor_model.predict(preprocessed_image)\n",
        "\n",
        "print(f\"\\nShape of extracted feature map: {feature_map.shape}\")\n",
        "\n",
        "# Clean up dummy image if created\n",
        "import os\n",
        "if os.path.exists(image_path):\n",
        "    try:\n",
        "        os.remove(image_path)\n",
        "    except OSError as e:\n",
        "        print(f\"Error removing dummy image: {e}\")\n",
        "\"\"\"\n",
        "0 conv2d_56 N/A\n",
        "1 conv2d_57 N/A\n",
        "2 max_pooling2d_23 N/A\n",
        "3 dropout_35 N/A\n",
        "4 conv2d_58 N/A\n",
        "5 conv2d_59 N/A\n",
        "6 max_pooling2d_24 N/A\n",
        "7 dropout_36 N/A\n",
        "8 conv2d_60 N/A\n",
        "9 conv2d_61 N/A\n",
        "10 max_pooling2d_25 N/A\n",
        "11 dropout_37 N/A\n",
        "12 conv2d_62 N/A\n",
        "13 conv2d_63 N/A\n",
        "14 final_pool_layer N/A\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "id": "ViMJvBJD2kHR",
        "outputId": "79244bfb-e713-474e-ba5b-935f07b036df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model.layers[0].output\n",
            "Using model.inputs[0]\n",
            "\n",
            "--- Feature Extractor Model Summary ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"feature_extractor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"feature_extractor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_56 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_57 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_23 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_58 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_59 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_24 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_61 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_25 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_62 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_63 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ final_pool_layer (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ final_pool_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,172,256\u001b[0m (4.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,172,256</span> (4.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,172,256\u001b[0m (4.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,172,256</span> (4.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step\n",
            "\n",
            "Shape of extracted feature map: (1, 4, 4, 256)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n0 conv2d_56 N/A\\n1 conv2d_57 N/A\\n2 max_pooling2d_23 N/A\\n3 dropout_35 N/A\\n4 conv2d_58 N/A\\n5 conv2d_59 N/A\\n6 max_pooling2d_24 N/A\\n7 dropout_36 N/A\\n8 conv2d_60 N/A\\n9 conv2d_61 N/A\\n10 max_pooling2d_25 N/A\\n11 dropout_37 N/A\\n12 conv2d_62 N/A\\n13 conv2d_63 N/A\\n14 final_pool_layer N/A\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"\\nExtracted feature map: {feature_map}\")\n",
        "\n",
        "import numpy as np\n",
        "import os # For checking file existence\n",
        "import sys\n",
        "\n",
        "log_filename = \"inference_log.txt\"\n",
        "print(f\"Attempting to write feature map details to '{log_filename}'...\")\n",
        "\n",
        "\n",
        "# Make sure feature_map exists before proceeding\n",
        "if 'feature_map' not in locals() and 'feature_map' not in globals():\n",
        "    print(\"Error: 'feature_map' variable not found. Please ensure it was generated.\")\n",
        "else:\n",
        "    # --- WARNING ---\n",
        "    total_elements = np.prod(feature_map.shape)\n",
        "    print(f\"[WARNING] Feature map has {total_elements} elements.\")\n",
        "    if total_elements > 10000: # Arbitrary threshold for warning\n",
        "        print(f\"[WARNING] Writing all elements might create a very large log file (> {total_elements * 5 // 1024} KB approx) and could be slow.\")\n",
        "    # -------------\n",
        "\n",
        "    try:\n",
        "        with open(log_filename, 'w') as log_file: # Use 'w' to overwrite each time for full logs\n",
        "            log_file.write(\"--- Full Extracted Feature Map ---\\n\")\n",
        "            log_file.write(f\"Timestamp: {np.datetime64('now')}\\n\")\n",
        "            log_file.write(f\"Shape: {feature_map.shape}\\n\")\n",
        "            log_file.write(f\"Data Type: {feature_map.dtype}\\n\")\n",
        "            log_file.write(f\"Min value: {np.min(feature_map)}\\n\")\n",
        "            log_file.write(f\"Max value: {np.max(feature_map)}\\n\")\n",
        "            log_file.write(f\"Mean value: {np.mean(feature_map)}\\n\")\n",
        "            log_file.write(f\"Std Dev: {np.std(feature_map)}\\n\")\n",
        "            log_file.write(\"\\nFeature Map Content (Full):\\n\")\n",
        "\n",
        "            # Option 2 (Modified): Write the FULL array content\n",
        "            # Use threshold=sys.maxsize or np.inf to disable summarization\n",
        "            feature_map_string = np.array2string(\n",
        "                feature_map,\n",
        "                precision=6,          # Increase precision if needed\n",
        "                suppress_small=False, # Show small numbers accurately\n",
        "                max_line_width=150,   # Wider lines might help for large arrays\n",
        "                threshold=sys.maxsize # Force printing the entire array\n",
        "                # threshold=np.inf   # Alternative way to force printing all\n",
        "            )\n",
        "            log_file.write(feature_map_string)\n",
        "\n",
        "            log_file.write(\"\\n--- End Full Feature Map ---\\n\")\n",
        "\n",
        "        print(f\"Successfully wrote FULL feature map to '{log_filename}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to log file '{log_filename}': {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNOCQxMlBGff",
        "outputId": "3d396ba7-3206-4101-b4f7-b2c1f20b6e70"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to write feature map details to 'inference_log.txt'...\n",
            "Error: 'feature_map' variable not found. Please ensure it was generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Create the \"Classifier Head\" Model ---\n",
        "\n",
        "# Find the index of the layer RIGHT AFTER our feature extraction layer ('final_pool_layer')\n",
        "# This is where the classifier head starts.\n",
        "feature_layer_name = 'final_pool_layer'\n",
        "try:\n",
        "    # Find the index of the layer by name\n",
        "    feature_layer_index = [i for i, layer in enumerate(model.layers) if layer.name == feature_layer_name][0]\n",
        "    classifier_start_index = feature_layer_index + 1\n",
        "    print(f\"Feature extraction layer ('{feature_layer_name}') index: {feature_layer_index}\")\n",
        "    print(f\"Classifier head starts at layer index: {classifier_start_index}\")\n",
        "except IndexError:\n",
        "    print(f\"Error: Could not find layer named '{feature_layer_name}' in the original model.\")\n",
        "    # Handle error appropriately\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred finding the layer index: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Define an Input layer matching the shape of the feature map (excluding batch dimension)\n",
        "feature_map_shape = feature_map.shape[1:]\n",
        "classifier_input = Input(shape=feature_map_shape, name=\"feature_map_input\")\n",
        "\n",
        "# connect\n",
        "x = classifier_input\n",
        "for i in range(classifier_start_index, len(model.layers)):\n",
        "    original_layer = model.layers[i]\n",
        "    x = original_layer(x, training=False)\n",
        "    print(f\"Added layer {i}: {original_layer.name}\")\n",
        "\n",
        "classifier_model = Model(inputs=classifier_input, outputs=x, name=\"classifier_head\")\n",
        "\n",
        "print(\"\\n--- Classifier Head Model Summary ---\")\n",
        "classifier_model.summary()\n",
        "\n",
        "\"\"\"\n",
        "15 dropout_33 N/A\n",
        "16 flatten_6 N/A\n",
        "17 dense_18 N/A\n",
        "18 dense_19 N/A\n",
        "19 dropout_34 N/A\n",
        "20 dense_20 N/A\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "5EKGHNyn5qzw",
        "outputId": "0aafa970-6c3e-4dc2-dd40-25b05d7494f3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction layer ('final_pool_layer') index: 14\n",
            "Classifier head starts at layer index: 15\n",
            "Added layer 15: dropout_38\n",
            "Added layer 16: flatten_7\n",
            "Added layer 17: dense_21\n",
            "Added layer 18: dense_22\n",
            "Added layer 19: dropout_39\n",
            "Added layer 20: dense_23\n",
            "\n",
            "--- Classifier Head Model Summary ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"classifier_head\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"classifier_head\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ feature_map_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_38 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,048,832\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ feature_map_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,081,986\u001b[0m (4.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,081,986</span> (4.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,081,986\u001b[0m (4.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,081,986</span> (4.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n15 dropout_33 N/A\\n16 flatten_6 N/A\\n17 dense_18 N/A\\n18 dense_19 N/A\\n19 dropout_34 N/A\\n20 dense_20 N/A\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_predictions = classifier_model.predict(feature_map)\n",
        "\n",
        "print(f\"\\nShape of final predictions: {final_predictions.shape}\") # Should be (1, num_classes)\n",
        "print(f\"Raw predictions (probabilities):\\n{final_predictions}\")\n",
        "\n",
        "predicted_class_index = np.argmax(final_predictions, axis=1)[0]\n",
        "print(f\"\\nPredicted class index: {predicted_class_index}\")\n",
        "\n",
        "print(\"\\n--- Verifying with original model ---\")\n",
        "\n",
        "# (Need to load the image again if not available)\n",
        "image_path = '/kaggle/split_data/test/Cat/11530.jpg'\n",
        "\n",
        "if not os.path.exists(image_path):\n",
        "    try:\n",
        "        print(\"Creating dummy image for verification...\")\n",
        "        dummy_data = np.random.rand(input_shape[0], input_shape[1], input_shape[2]) * 255\n",
        "        tf.keras.preprocessing.image.save_img(image_path, dummy_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create dummy image: {e}.\")\n",
        "else:\n",
        "    def load_and_preprocess_image(image_path, target_shape):\n",
        "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_shape[:2])\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0\n",
        "        return img_array\n",
        "\n",
        "    preprocessed_image_for_full_model = load_and_preprocess_image(image_path, input_shape)\n",
        "    original_model_predictions = model.predict(preprocessed_image_for_full_model)\n",
        "    original_predicted_class_index = np.argmax(original_model_predictions, axis=1)[0]\n",
        "\n",
        "    print(f\"Original model predicted index: {original_predicted_class_index}\")\n",
        "    print(f\"Feature map pipeline predicted index: {predicted_class_index}\")\n",
        "\n",
        "    # Check if predictions are numerically close (they should be almost identical)\n",
        "    if np.allclose(final_predictions, original_model_predictions, atol=1e-6):\n",
        "        print(\"SUCCESS: Predictions from split model match original model (before compression).\")\n",
        "    else:\n",
        "        print(\"WARNING: Predictions from split model DO NOT match original model.\")\n",
        "        print(\"Original:\", original_model_predictions)\n",
        "        print(\"Split:\", final_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh6ZWxfM60or",
        "outputId": "aaf382f6-d89e-480e-ff4e-a6cf277a12bb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\n",
            "Shape of final predictions: (1, 2)\n",
            "Raw predictions (probabilities):\n",
            "[[0.75012386 0.24987611]]\n",
            "\n",
            "Predicted class index: 0\n",
            "\n",
            "--- Verifying with original model ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Original model predicted index: 0\n",
            "Feature map pipeline predicted index: 0\n",
            "WARNING: Predictions from split model DO NOT match original model.\n",
            "Original: [[0.7234268  0.27657315]]\n",
            "Split: [[0.75012386 0.24987611]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFrsDVvSV8_S"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}